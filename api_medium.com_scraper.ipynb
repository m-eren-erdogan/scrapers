{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03a1592b",
   "metadata": {},
   "source": [
    "## Medium.com Article Scraper with API\n",
    "\n",
    "This scraper utilizes a private API by rapidapi.com, and scrapes medium.com articles with given category tags. It filters for non-membership articles and applies a mean article read time criteria to disqualify short articles with just a few sentences. It also removes personal information such as email addresses, and then saves the article content in a json format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3676911f",
   "metadata": {},
   "source": [
    " ### Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c1fd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import http.client\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b69d166",
   "metadata": {},
   "source": [
    "### API Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167858ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "rapidapi_key0 = os.getenv(\"rapidapi_key0\")\n",
    "rapidapi_key1 = os.getenv(\"rapidapi_key1\")\n",
    "rapidapi_key2 = os.getenv(\"rapidapi_key2\")\n",
    "rapidapi_key3 = os.getenv(\"rapidapi_key3\")\n",
    "rapidapi_key4 = os.getenv(\"rapidapi_key4\")\n",
    "rapidapi_host = \"medium2.p.rapidapi.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dcb15f",
   "metadata": {},
   "source": [
    "### Extracting Article IDs Through the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ebb297",
   "metadata": {},
   "outputs": [],
   "source": [
    "related_tags = ['artificial-intelligence', 'machine-learning', 'deep-learning']\n",
    "\n",
    "# Create an empty list to store article ID's\n",
    "medium_article_ids = []\n",
    "\n",
    "# Loop through the tags to get related ID's\n",
    "for tag in related_tags:\n",
    "    conn = http.client.HTTPSConnection(\"medium2.p.rapidapi.com\")\n",
    "    headers = {\n",
    "        'X-RapidAPI-Key': rapidapi_key0,\n",
    "        'X-RapidAPI-Host': rapidapi_host\n",
    "    }\n",
    "    # Create the URL for the tag search\n",
    "    url = f'/search/articles?query={tag}'\n",
    "    conn.request(\"GET\", url, headers=headers)\n",
    "    res = conn.getresponse()\n",
    "    data = res.read()\n",
    "    data_json = json.loads(data.decode(\"utf-8\"))\n",
    "    \n",
    "    # Attach the acquired ID's to the list\n",
    "    articles_list = data_json.get('articles', [])\n",
    "    medium_article_ids.extend(articles_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b400f3",
   "metadata": {},
   "source": [
    "### Removing Duplicates and Counting Unique Article IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8d0ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the original count of article IDs\n",
    "original_count = len(medium_article_ids)\n",
    "\n",
    "# Create a set to store unique article IDs, effectively removing duplicates\n",
    "unique_article_ids = set(medium_article_ids)\n",
    "\n",
    "# Calculate the count of unique article IDs\n",
    "unique_count = len(unique_article_ids)\n",
    "\n",
    "# Calculate the count of dropped (duplicate) article IDs\n",
    "dropped_count = original_count - unique_count\n",
    "\n",
    "print(f\"Original count: {original_count}\")\n",
    "print(f\"Unique count: {unique_count}\")\n",
    "print(f\"Dropped count: {dropped_count}\")\n",
    "\n",
    "# Save the data as a JSON file\n",
    "with open(\"medium_article_ids.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(medium_article_ids, json_file, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01307dd",
   "metadata": {},
   "source": [
    "### Extracting Article Information (Reading Time and Member-only Status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4ade77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "# List of API keys\n",
    "api_keys = [rapidapi_key1, rapidapi_key2, rapidapi_key3]\n",
    "\n",
    "# Number of articles to get for each API key\n",
    "articles_per_key = 150\n",
    "\n",
    "# Create a starting point for the keys\n",
    "start_index = 0\n",
    "\n",
    "# Create a list to store article information for all API keys\n",
    "all_article_information = []\n",
    "\n",
    "# Open the JSON file for writing within the 'with' statement\n",
    "with open(\"article_information.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "    \n",
    "    # Call different \n",
    "    for api_key in api_keys:\n",
    "        # Create counters for each API key\n",
    "        calls_processed = 0\n",
    "        calls_failed = 0\n",
    "\n",
    "        # Loop through the article IDs for the current API key\n",
    "        for i in range(start_index, start_index + articles_per_key):\n",
    "            if i >= len(medium_article_ids):\n",
    "                break  # Stop if we've fetched 150 articles with a single key\n",
    "\n",
    "            article_id = medium_article_ids[i]\n",
    "            url = f\"https://medium2.p.rapidapi.com/article/{article_id}\"\n",
    "\n",
    "            headers = {\n",
    "                \"X-RapidAPI-Key\": api_key,\n",
    "                \"X-RapidAPI-Host\": rapidapi_host\n",
    "            }\n",
    "\n",
    "            response = requests.get(url, headers=headers)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                article_data = response.json()\n",
    "                reading_time = article_data.get(\"reading_time\", \"N/A\")\n",
    "                is_lock = article_data.get(\"is_locked\")\n",
    "\n",
    "                article_info = {\n",
    "                    \"article_id\": article_id, \n",
    "                    \"reading_time\": reading_time, \n",
    "                    \"member-only\": is_lock\n",
    "                }\n",
    "                    \n",
    "                all_article_information.append(article_info)  # Append to the list\n",
    "\n",
    "                # Print calls_processed within the loop\n",
    "                print(f\"API Key {api_key}: Processed {calls_processed} articles\")\n",
    "\n",
    "                calls_processed += 1\n",
    "            else:\n",
    "                print(f\"Failed to fetch article {article_id} with API key {api_key}. Status code: {response.status_code}\")\n",
    "                calls_failed += 1\n",
    "\n",
    "        print(f\"API Key {api_key}: Processed {calls_processed} articles, Failed to fetch {calls_failed} articles\")\n",
    "\n",
    "        # Update the starting point for the next key\n",
    "        start_index += articles_per_key\n",
    "\n",
    "# Save the combined data as a single JSON file\n",
    "with open(\"article_information.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(all_article_information, json_file, indent=4)\n",
    "\n",
    "print(f\"Total processed: {len(all_article_information)} articles\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce3fd32",
   "metadata": {},
   "source": [
    "### Filtering the Article Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b1fc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data in\n",
    "with open(\"article_information.json\", \"r\", encoding=\"utf-8\") as json_file:\n",
    "    medium_articles_info = json.load(json_file)\n",
    "\n",
    "total_reading_time = 0\n",
    "\n",
    "# For loop to retrieve reading time and save it to total reading time object\n",
    "for article in medium_articles_info:\n",
    "    reading_time = article.get('reading_time', 0)\n",
    "    total_reading_time += float(reading_time)\n",
    "\n",
    "# Calculate the mean reading time\n",
    "mean_reading_time = total_reading_time / len(medium_articles_info) if len(medium_articles_info) > 0 else 0\n",
    "\n",
    "print(f\"Mean Reading Time: {mean_reading_time} minutes\")\n",
    "\n",
    "# Filter out articles with reading time lesser than the mean\n",
    "filtered_articles = [article for article in medium_articles_info if float(article[\"reading_time\"]) > mean_reading_time and not article[\"member-only\"]]\n",
    "\n",
    "# Print filtered articles\n",
    "print(\"Filtered Articles:\")\n",
    "for article in filtered_articles:\n",
    "    print(f\"Article ID: {article['article_id']}, Reading Time: {article['reading_time']} minutes, \"\n",
    "          f\"Members-only: {article['member-only']}\")\n",
    "\n",
    "# Get the amount of articles after filtering\n",
    "print(len(filtered_articles))\n",
    "\n",
    "# Save the data as a JSON file\n",
    "with open(\"filtered_article_info.json\", \"w\") as json_file:\n",
    "    json.dump(filtered_articles, json_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1085cfe8",
   "metadata": {},
   "source": [
    "### Extracting Article Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba33a577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from a JSON file containing filtered article information\n",
    "with open(\"filtered_article_info.json\", \"r\", encoding=\"utf-8\") as json_file:\n",
    "    filtered_articles = json.load(json_file)\n",
    "\n",
    "# Initialize an empty list to store article content\n",
    "article_contents = []\n",
    "\n",
    "# Open the JSON file for writing the article content\n",
    "with open(\"article_contents.json\", \"w\", encoding=\"utf-8\") as content_json_file:\n",
    "    # Loop through the filtered articles\n",
    "    for article_info in filtered_articles:\n",
    "        article_id = article_info['article_id']\n",
    "\n",
    "        # Construct the URL to fetch the article content\n",
    "        url = f'https://medium2.p.rapidapi.com/article/{article_id}/content'\n",
    "\n",
    "        # Set the headers for the HTTP request, including the API key and host\n",
    "        headers = {\n",
    "            \"X-RapidAPI-Key\": rapidapi_key0,\n",
    "            \"X-RapidAPI-Host\": rapidapi_host\n",
    "        }\n",
    "\n",
    "        # Make an HTTP GET request to retrieve the article content\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        # Check if the response status code is 200 (successful request)\n",
    "        if response.status_code == 200:\n",
    "            # Parse the JSON response to extract the article content\n",
    "            article_data = response.json()\n",
    "            article_content = article_data.get('content')\n",
    "\n",
    "            # Use a regular expression pattern to remove email addresses from the article content\n",
    "            pattern = r'\\S+@\\S+'\n",
    "            article_content = re.sub(pattern, '', article_content)\n",
    "\n",
    "            # Create a dictionary with article ID and its content, and add it to the article_contents list\n",
    "            article_content_all = {\n",
    "                \"article_id\": article_id,\n",
    "                \"article_content\": article_content\n",
    "            }\n",
    "            article_contents.append(article_content_all)\n",
    "\n",
    "            # Write the info to the JSON file\n",
    "            content_json_file.write(json.dumps(article_content_all, indent=4))\n",
    "            content_json_file.write('\\n')\n",
    "        else:\n",
    "            # Print an error message for failed requests\n",
    "            print(f\"Failed to fetch article {article_id}. Status code: {response.status_code}\")\n",
    "\n",
    "# Create a dictionary to map each article ID to its content\n",
    "article_data_dict = {article_info['article_id']: article_info['article_content'] for article_info in article_contents}\n",
    "\n",
    "# Save the dictionary as a JSON file \n",
    "with open(\"article_contents.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(article_data_dict, json_file, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505d6355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the list to see content\n",
    "print(article_data_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
