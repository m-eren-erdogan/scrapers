{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd0f38b9",
   "metadata": {},
   "source": [
    "# Web Scraping With Scheduling-Cron: Beginners' Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430ccac4",
   "metadata": {},
   "source": [
    "This notebook tries to understand the core coding concepts of web scraping, and do one's best to introduce scheduling-cron to it.\n",
    "\n",
    "It will follow three main steps:\n",
    "\n",
    "1. Building the scraper,\n",
    "\n",
    "2. Figuring out the scheduling-cron and\n",
    "\n",
    "3. Doing some basic explarotary analysis with pandas.\n",
    "\n",
    "The notebook contains commentary on the codes. It can be a little hard to read, if yes, we are open to suggestions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf77c304",
   "metadata": {},
   "source": [
    "## Building the Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1731f483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets load the packages\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e58f2bb",
   "metadata": {},
   "source": [
    "### Scraper part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1a5fd17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"artists\":[{\"artist\":\"Wordsworth feat. Adanita Ross\",\"artist_id\":\"ARJ66JQ1187B99D2FF\"},{\"artist\":\"Don & Juan\",\"artist_id\":\"ARAVPU21187B993957\"},{\"artist\":\"Billy May & His Orchestra\",\"artist_id\":\"AR7SVRP1187FB55875\"},{\"artist\":\"Iggy Pop\",\"artist_id\":\"AR5OH2Z1187B9A46A9\"},{\"artist\":\"The Honeydogs\",\"artist_id\":\"ARSWORN1187B991A7B\"}]}\n"
     ]
    }
   ],
   "source": [
    "url = 'https://api.music-to-scrape.org/artists/featured' # We are extracting the featured artists via music-to-scrape API. \n",
    "\n",
    "# The URL can be found within the Featured Artist header on the left of the API documentation.\n",
    "\n",
    "header = {'User-agent': 'Mozilla/5.0'} # We are impersonating as a Mozilla browser.\n",
    "web_request = requests.get(url, headers = header) # We are getting the info about featured artists into the web_request object.\n",
    "web_request.encoding = web_request.apparent_encoding # We are setting encoding to UTF-8.\n",
    "\n",
    "## This ensures enables us to read non-English characters too.\n",
    "\n",
    "web_request_source_code = web_request.text # We are extracting the text data in the web_request object.\n",
    "\n",
    "print(web_request_source_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8c5bb0",
   "metadata": {},
   "source": [
    "Below, we are prototyping a small scraper. It scrapes the featured artists from the website **each** second. It stops in **5 seconds**. \n",
    "\n",
    "We need to set a stop time for our while loop. Otherwise, it will run itself endlessly. Since we are prototyping, we decided to run the loop only for 5 seconds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f9441d",
   "metadata": {},
   "source": [
    "### Scraper part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2385e39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collecting featuring data... please wait.\n",
      "collecting featuring data... please wait.\n",
      "collecting featuring data... please wait.\n",
      "collecting featuring data... please wait.\n",
      "collecting featuring data... please wait.\n",
      "Printing featured artists: ['AREFUMW11F4C844D2B', 'AR19SOA1187B98F6E6', 'ARMOE2A1187B991F89', 'ARICCN811C8A41750F', 'AR0U44O1187B99007C', 'ARCQCYR1187B99367F', 'ARR2NH51187B98CE4C', 'ARMBTFC1187FB56343', 'AR5OH2Z1187B9A46A9', 'ARLYPKH1241B9C7185', 'ARK30AI1187FB3C777', 'ARYFAT91187B99FEF5', 'ARMBTFC1187FB56343', 'ARR2NH51187B98CE4C', 'ARQ76LG1187B9ACD84', 'ARR2NH51187B98CE4C', 'ARMORUX11F50C4EEBF', 'ARK30AI1187FB3C777', 'ARKIQSL1241B9C90C8', 'ARQQFIQ1187B99DB43', 'AR7KKE01187FB3D87B', 'ARY55LO1187B9A3F17', 'ARKIQSL1241B9C90C8', 'ARKGWMO11F50C4813F', 'ARQIWOW11F4C840FF1']\n"
     ]
    }
   ],
   "source": [
    "featured_artists = [] # We are defining an empty list of featured artists. It will be filled by the upcoming while loop.\n",
    "\n",
    "\n",
    "end_time = time.time() + 5 # This code here retrieves now's time as unix and adds 5 seconds to it.\n",
    "\n",
    "\n",
    "# When the while loop reaches the time of this end_time object, it will stop running.\n",
    "\n",
    "while time.time() <= end_time: # As long as end_time is bigger than or equal to the now's time,\n",
    "    print('collecting featuring data... please wait.')\n",
    "    url = 'https://api.music-to-scrape.org/artists/featured'\n",
    "    response = requests.get(url, headers=header)\n",
    "    json_response = response.json() \n",
    "    \n",
    "    for id in json_response['artists']: # We are looping through the json_response object's artists dictionary key.\n",
    "        featured_artists.append(id['artist_id']) # We are adding the extracted artist_id's to the empty feat\n",
    "    time.sleep(1)\n",
    "\n",
    "print(f'Printing featured artists: {featured_artists}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14532794",
   "metadata": {},
   "source": [
    "Since we are using API, response of the website is a json format. We can use the `.json()` method and easily and save the response from the server into a json_response object like above. Now we will loop through the values of **artists** key from the json_response object. Afterwards, we will expand the empty list at the top, `featured_artists` with the **artist_id** information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc52b94",
   "metadata": {},
   "source": [
    "The goal is to create a scraper that scrapes in **every** second, for **five** seconds. The delay provided above with the `time_sleep()` function enables that. Without it, code would've tried to get the complete data at once. \n",
    "\n",
    "This is not healthy for a server since it might get flooded with requests.\n",
    "\n",
    "Now only part left is to get artist infos based on the artist ids above and write a json file that contains this information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59f5383",
   "metadata": {},
   "source": [
    "### Scraper part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba19a7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AREFUMW11F4C844D2B\n",
      "AR19SOA1187B98F6E6\n",
      "ARMOE2A1187B991F89\n",
      "ARICCN811C8A41750F\n",
      "AR0U44O1187B99007C\n",
      "ARCQCYR1187B99367F\n",
      "ARR2NH51187B98CE4C\n",
      "ARMBTFC1187FB56343\n",
      "AR5OH2Z1187B9A46A9\n",
      "ARLYPKH1241B9C7185\n",
      "ARK30AI1187FB3C777\n",
      "ARYFAT91187B99FEF5\n",
      "ARMBTFC1187FB56343\n",
      "ARR2NH51187B98CE4C\n",
      "ARQ76LG1187B9ACD84\n",
      "ARR2NH51187B98CE4C\n",
      "ARMORUX11F50C4EEBF\n",
      "ARK30AI1187FB3C777\n",
      "ARKIQSL1241B9C90C8\n",
      "ARQQFIQ1187B99DB43\n",
      "AR7KKE01187FB3D87B\n",
      "ARY55LO1187B9A3F17\n",
      "ARKIQSL1241B9C90C8\n",
      "ARKGWMO11F50C4813F\n",
      "ARQIWOW11F4C840FF1\n"
     ]
    }
   ],
   "source": [
    "f = open('artist_info_sc.json', 'a', encoding = 'utf-8') # We are opening a json file to write the extracted information.\n",
    "\n",
    "for artist in featured_artists: # We are pasting the retrieved artist id's into the URL below to get the artist infos.\n",
    "    print(artist)\n",
    "    url = f'https://api.music-to-scrape.org/artist/info?artistid={artist}' # We are adding artist ids here.\n",
    "    response = requests.get(url, headers=header)\n",
    "    json_response = response.json()\n",
    "    json_response['retrieval_unix']=time.time() # We are creating this 'retrieval_unix' key first. \n",
    "\n",
    "# We save the time that the info is retrieved into this key as a value. This way, we can keep track when the info is acquired.\n",
    "\n",
    "    f.write(json.dumps(json_response)) # We are obtaining the json file.\n",
    "    f.write('\\n')\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfeb9cb",
   "metadata": {},
   "source": [
    "## Scheduling-Cron for macOS / Linux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc29b92",
   "metadata": {},
   "source": [
    "After our scraper is done, we need to dwell into the **Scheduling-Cron**. But first, what do they actually mean?\n",
    "\n",
    "Scheduling in this sense means to run our scraper at a specific time. Remember that the scraper above runs for *5 seconds*, *every second*. We can manipulate this and ask for it to run for *2 hours*, in *every 10 minutes*, at for example *20:00*, without changing the code itself. In addition, we can tell the computer to perform this action at whichever time we want. \n",
    "\n",
    "So telling the computer to run the code at **20:00 for 2 hours in every 10 minutes** is scheduling.\n",
    "\n",
    "Cron is the method we use to make the scheduling happen. We want the python script to run in a given time, in given intervals. Therefore we need to program the computer to do so. For example, when you want to run the script during the night, you can just use Cron to handle this. You can leave your PC open, and schedule a scraping task thanks to the Cron."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75b6e74",
   "metadata": {},
   "source": [
    "To explain Cron syntax [Tilburg Science Hub Tutorial](https://tilburgsciencehub.com/building-blocks/automate-and-execute-your-work/automate-your-workflow/task-scheduling/?utm_campaign=referral-short) for Cron."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e998e9",
   "metadata": {},
   "source": [
    " ### Cron syntax for macOS / Linux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72947f58",
   "metadata": {},
   "source": [
    "Cron needs a specific syntax to run in mac and Linux. The syntax consists of *5* different symbols. \n",
    "\n",
    "```\n",
    "*    *    *    *    *  \n",
    "┬    ┬    ┬    ┬    ┬\n",
    "│    │    │    │    └─  Weekday  (0 - 6)\n",
    "│    │    │    └──────  Month    (1 - 12)\n",
    "│    │    └───────────  Day      (1 - 31)\n",
    "│    └────────────────  Hour     (0 - 23)\n",
    "└─────────────────────  Minute   (0 - 59)\n",
    "```\n",
    "\n",
    "So if we were to run a script in every 5th and 55th minutes of an hour, we should construct it as: 5,55 * * * *.\n",
    "\n",
    "Some examples from the [Tilburg Science Hub Tutorial](https://tilburgsciencehub.com/building-blocks/automate-and-execute-your-work/automate-your-workflow/task-scheduling/?utm_campaign=referral-short)\n",
    "\n",
    "- 30 * * * * = every 30th minute of every hour every day\n",
    "- 30 5 * * * = at 05:30 on every day\n",
    "- 30 5 1 * * = at 05:30 the first day of the month\n",
    "- 30 5 1 1 * = at 05:30 on January 1st\n",
    "- 30 5 * * 1 = at 05:30 on every Monday\n",
    "- 0 0 */3 * * = every 3 days at midnight\n",
    "- 30 5 1,15 * * = at 05:30 on every 1st and 15th day of the month\n",
    "- */30 9-17 * * 1-5 = every 30 minutes during business hours\n",
    "\n",
    "There is a great website called [CronGuru](https://crontab.guru) that explains the Cron Syntax. Check it out for more explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e463cfea",
   "metadata": {},
   "source": [
    "#### Important Tip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052b6fee",
   "metadata": {},
   "source": [
    "In our case, we needed to give \"Full Disk Access\" permission to Cron from System Settings.\n",
    "\n",
    "This [stack overflow discussion](URL) will guide you through.\n",
    "\n",
    "\n",
    "To add the exception:\n",
    "\n",
    "1. click the + button\n",
    "2. hit ⌘⇧G\n",
    "3. enter /usr/sbin\n",
    "4. double click the cron file.\n",
    "\n",
    "If you can see the cron clicked in the security & privacy settings: Perfect! Now you should be able to gave a permission for your cron tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b4312a",
   "metadata": {},
   "source": [
    "### Implementing the Required 2 Hours every 10 Minutes Scheduling-Cron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a616177",
   "metadata": {},
   "source": [
    "Let's follow the steps from [Tilburg Science Hub Tutorial](https://tilburgsciencehub.com/building-blocks/automate-and-execute-your-work/automate-your-workflow/task-scheduling/?utm_campaign=referral-short):\n",
    "\n",
    "1. `crontab -l` to have a look if there are any cron jobs settled.\n",
    "\n",
    "If there are none, let's set one with:\n",
    "\n",
    "2. `crontab -e` \n",
    "3. `Press I` so that you can edit the text.\n",
    "4. Insert the following syntax *always specify the full path!*\n",
    "\n",
    "`<CRON CODE> <PATH OF YOUR PYTHON INSTALLATION> <PATH TO PYTHON FILE>`\n",
    "\n",
    "Example: `<*/10 20,21 * * *> </Users/vscanturk/anaconda3/bin/python> </Users/vscanturk/Desktop/collabscript5.py>`\n",
    "\n",
    "This will run the code in every 10 minutes for the hours of **20:00** & **21:00**. This way, we'll be able to accomplish what the exercise asks us, running the python script in 2 hours every 10 minutes.\n",
    "\n",
    "\n",
    "5. Press `Esc` and type `:wq` followed by Enter to save your changes (if a window pops up, select OK).\n",
    "\n",
    "If you run the `crontab -l` now, your newly scheduled task should be listed. We recommend keeping an eye out whether your scheduler works as expected, especially in the beginning. If not, you can make changes to the cron file at any time. To remove all existing tasks and clean up the cron filetype, use `crontab -r`.\n",
    "\n",
    "There are 5 featured artists in the website and our Python retrieves them in every second for 5 seconds. So each time the code runs, we end up with 25 artists. In two hours for every 10 minutes, it will retrieve `12 * 25 = 300` artists.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c5c4f9",
   "metadata": {},
   "source": [
    "## pandas and Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38fd91b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading the data.\n",
    "\n",
    "# Specify the path to your JSON file\n",
    "\n",
    "json_file_path = \"artists_info_sc.json\" # This is the file that contains 300 artists.\n",
    "\n",
    "# Read the JSON file line by line and process each JSON object separately\n",
    "data = []\n",
    "with open(json_file_path, \"r\") as json_file:\n",
    "    for line in json_file:\n",
    "        json_data = json.loads(line.strip())\n",
    "        data.append(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89abef20",
   "metadata": {},
   "source": [
    "ChatGPT says that the **with** clause in the beginning helps Python to open the data in reading mode -that's the *r* in the open function. We tend to forget to close the data we've opened. When the opening and closing process is dealt via **with** clause, and data is opened in a reading mode, whole process is safer for both data and Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3467cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = pd.DataFrame(data) # Now we are changing the imported json file to a data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b895b659",
   "metadata": {},
   "source": [
    "Our object is a **pandas dataframe** now. \n",
    "\n",
    "Following functions are all pandas functions, which work well with pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69ef63a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique artists captured: 38\n",
      "Number of duplicate artists captured: 187\n",
      "Start timestamp of the scraper: 1694901607.229821\n",
      "End timestamp of the scraper: 1694905011.8462589\n",
      "artistname\n",
      "A Challenge Of Honour               189.0\n",
      "Arsonists Get All The Girls          88.0\n",
      "Art Ensemble Of Chicago              43.0\n",
      "Billy May & His Orchestra           341.0\n",
      "Black Flag                          489.0\n",
      "Bob Neuwirth                        527.0\n",
      "Chris Camozzi                       201.0\n",
      "Cumberland Quartet                  103.0\n",
      "Dive                                188.0\n",
      "Don & Juan                           62.0\n",
      "Eric B. & Rakim                      91.0\n",
      "Fred Merpol                         163.0\n",
      "Furry Lewis                        1209.0\n",
      "Grant Geissman                      191.0\n",
      "Greek Gypsy Musicians                89.0\n",
      "Iggy Pop                            496.0\n",
      "Jonathan Edwards / Will Ferrell      17.0\n",
      "Kate Clinton                        214.0\n",
      "Ketty DB                            199.0\n",
      "Korn                                477.0\n",
      "Liquid Spill                        135.0\n",
      "Lynette Schultz                      99.0\n",
      "Milo                                157.0\n",
      "O'2L                                198.0\n",
      "Off Broadway                        109.0\n",
      "Paul Brown                          315.0\n",
      "Republica;Jonny Male;Dave Arch      253.0\n",
      "Roger McGuinn                         1.0\n",
      "Russell Malone                       27.0\n",
      "Tam Tam Go!                        1179.0\n",
      "Terry Muska                         285.0\n",
      "The Honeydogs                        80.0\n",
      "The Meatmen                         424.0\n",
      "This Moment                         342.0\n",
      "Three-6 Mafia                       266.0\n",
      "Tigertailz                          151.0\n",
      "Wordsworth feat. Adanita Ross       381.0\n",
      "Ya Boy                               76.0\n",
      "Name: total_plays, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 1. Number of unique artists captured\n",
    "\n",
    "unique_artists_count = data_frame['artistname'].nunique() # This nunique() method returns unique values from given column.\n",
    "\n",
    "print(f\"Number of unique artists captured: {unique_artists_count}\")\n",
    "\n",
    "# 2. Number of duplicate artists captured\n",
    "\n",
    "duplicate_artists_count = len(data_frame) - unique_artists_count # This is the count of all artists minus the unique ones.\n",
    "\n",
    "print(f\"Number of duplicate artists captured: {duplicate_artists_count}\")\n",
    "\n",
    "# 3. Start and end timestamp of process\n",
    "\n",
    "start_timestamp = data_frame['retrieval_unix'].min() # min() method gets the start time of the scraping.\n",
    "\n",
    "print(f\"Start timestamp of the scraper: {start_timestamp}\")\n",
    "\n",
    "end_timestamp = data_frame['retrieval_unix'].max() # Similar here, max() gets the end of it.\n",
    "\n",
    "print(f\"End timestamp of the scraper: {end_timestamp}\")\n",
    "\n",
    "# 4. Average number of plays for featured artists\n",
    "\n",
    "average_plays_for_featured_artists = data_frame.groupby('artistname')['total_plays'].mean()\n",
    "\n",
    "# We group the database by artist names and get the mean of their total plays.\n",
    "\n",
    "print(average_plays_for_featured_artists)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
